# æ”¯æŒçš„æ¨¡å‹é…ç½®å‚è€ƒ

## å¿«é€ŸæŸ¥è¯¢è¡¨

| æ¨¡å‹ç³»åˆ— | ç¤ºä¾‹æ¨¡å‹ | ç±»å‹ | æ¨èå‚æ•° |
|---------|--------|------|--------|
| Flan-T5 | google/flan-t5-base | Encoder-Decoder | num_child=3, k=10 |
| T5 | google/t5-base | Encoder-Decoder | num_child=3, k=10 |
| LLaMA | meta-llama/Llama-2-7b-hf | Decoder-Only | num_child=2, k=10 |
| Mistral | mistralai/Mistral-7B-Instruct-v0.1 | Decoder-Only | num_child=2, k=10 |
| Phi | microsoft/phi-2 | Decoder-Only | num_child=2, k=10 |
| Qwen | Qwen/Qwen-7B-Chat | Decoder-Only | num_child=2, k=10 |
| Baichuan | baichuan-inc/Baichuan2-7B-Chat | Decoder-Only | num_child=2, k=10 |
| StarCoder | bigcode/starcoder | Decoder-Only | num_child=2, k=10 |

## æŒ‰å¤§å°åˆ†ç±»

### æå°å‹ (< 1B)
```python
# æœ€å¿«çš„æ¨ç†
ranker = UniversalSetwiseLlmRanker("google/flan-t5-small")
```

### å°å‹ (1-7B)
```python
# æ¨èç”¨äºå®æ—¶åº”ç”¨
ranker = UniversalSetwiseLlmRanker("google/flan-t5-base")
ranker = UniversalSetwiseLlmRanker("microsoft/phi-2")
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-7b-chat-hf")
```

### ä¸­å‹ (7-13B)
```python
# æ¨èç”¨äºç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯
ranker = UniversalSetwiseLlmRanker("google/flan-t5-large")
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-13b-chat-hf")
ranker = UniversalSetwiseLlmRanker("mistralai/Mistral-7B-Instruct-v0.1")
```

### å¤§å‹ (13-70B)
```python
# æœ€é«˜ç²¾åº¦ï¼Œéœ€è¦è¶³å¤ŸGPUå†…å­˜
ranker = UniversalSetwiseLlmRanker("google/flan-t5-xl")
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-70b-chat-hf")
```

### è¶…å¤§å‹ (> 70B)
```python
# éœ€è¦å¤šGPUæˆ–é‡åŒ–
ranker = UniversalSetwiseLlmRanker("google/flan-t5-xxl")
# ä½¿ç”¨8-bitæˆ–4-bité‡åŒ–
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-70b-chat-hf")
```

## æŒ‰ç”¨é€”åˆ†ç±»

### é€šç”¨æŒ‡ä»¤è·Ÿéš
```python
# æ¨èFlan-T5ï¼ˆå¾®è°ƒç”¨äºè·ŸéšæŒ‡ä»¤ï¼‰
ranker = UniversalSetwiseLlmRanker("google/flan-t5-base")
ranker = UniversalSetwiseLlmRanker("google/flan-t5-large")
```

### å¤šè¯­è¨€æ”¯æŒ
```python
# mT5æ”¯æŒ101ç§è¯­è¨€
ranker = UniversalSetwiseLlmRanker("google/mt5-base")
```

### ä¸­æ–‡æ”¯æŒ
```python
ranker = UniversalSetwiseLlmRanker("Qwen/Qwen-7B-Chat")
ranker = UniversalSetwiseLlmRanker("baichuan-inc/Baichuan2-7B-Chat")
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-7b-chinese-chat-hf")
```

### ä»£ç ç›¸å…³
```python
ranker = UniversalSetwiseLlmRanker("bigcode/starcoder")
ranker = UniversalSetwiseLlmRanker("bigcode/starcoder2")
```

### å¯¹è¯ä¼˜åŒ–
```python
# Chatç‰ˆæœ¬é’ˆå¯¹å¯¹è¯åœºæ™¯ä¼˜åŒ–
ranker = UniversalSetwiseLlmRanker("meta-llama/Llama-2-7b-chat-hf")
ranker = UniversalSetwiseLlmRanker("mistralai/Mistral-7B-Instruct-v0.1")
```

## å®Œæ•´æ¨¡å‹åˆ—è¡¨

### Google T5ç³»åˆ—

**Encoder-Decoder (æ¨èç”¨äºranking)**

```python
# åŸºç¡€T5
models = [
    "google/t5-small",        # 60Må‚æ•°
    "google/t5-base",         # 220Må‚æ•°
    "google/t5-large",        # 770Må‚æ•°
    "google/t5-3b",           # 3Bå‚æ•°
    "google/t5-11b",          # 11Bå‚æ•°
]

# Flan-T5 (æŒ‡ä»¤å¾®è°ƒï¼Œæ¨è)
models = [
    "google/flan-t5-small",   # 80Må‚æ•°
    "google/flan-t5-base",    # 250Må‚æ•° â­ æ¨è
    "google/flan-t5-large",   # 780Må‚æ•°
    "google/flan-t5-xl",      # 3Bå‚æ•°
    "google/flan-t5-xxl",     # 11Bå‚æ•°
]

# mT5 (å¤šè¯­è¨€)
models = [
    "google/mt5-small",       # æ”¯æŒ101ç§è¯­è¨€
    "google/mt5-base",
    "google/mt5-large",
    "google/mt5-xl",
    "google/mt5-xxl",
]
```

### Meta LLaMAç³»åˆ—

**Decoder-Only**

```python
# LLaMA 2
models = [
    "meta-llama/Llama-2-7b-hf",           # Base
    "meta-llama/Llama-2-7b-chat-hf",      # Chatä¼˜åŒ– â­
    "meta-llama/Llama-2-13b-hf",
    "meta-llama/Llama-2-13b-chat-hf",     # Chatä¼˜åŒ–
    "meta-llama/Llama-2-70b-hf",
    "meta-llama/Llama-2-70b-chat-hf",     # Chatä¼˜åŒ–
]

# LLaMAå˜ç§
models = [
    "meta-llama/Llama-Chinese-7b",        # ä¸­æ–‡ä¼˜åŒ–
    "meta-llama/Llama-2-7b-chinese-chat-hf",
]
```

### Mistralç³»åˆ—

**Decoder-Only**

```python
models = [
    "mistralai/Mistral-7B-v0.1",          # Base
    "mistralai/Mistral-7B-Instruct-v0.1", # Instruct â­
    "mistralai/Mistral-7B-Instruct-v0.2",
    "mistralai/Mixtral-8x7B-Instruct-v0.1", # Mixture of Experts
]
```

### Microsoft Phiç³»åˆ—

**Decoder-Only (å°ä½†å¼ºå¤§)**

```python
models = [
    "microsoft/phi-1.5",                  # 1.3B
    "microsoft/phi-2",                    # 2.7B â­
    "microsoft/phi-3",                    # 3.8B
]
```

### é˜¿é‡ŒQwenç³»åˆ—

**Decoder-Only (æ”¯æŒä¸­æ–‡)**

```python
models = [
    "Qwen/Qwen-7B",                       # Base
    "Qwen/Qwen-7B-Chat",                  # Chat â­
    "Qwen/Qwen-14B",
    "Qwen/Qwen-14B-Chat",
    "Qwen/Qwen-72B",
    "Qwen/Qwen-72B-Chat",
]
```

### ç™¾å·Baichuanç³»åˆ—

**Decoder-Only (æ”¯æŒä¸­æ–‡)**

```python
models = [
    "baichuan-inc/Baichuan-7B",
    "baichuan-inc/Baichuan-13B-Base",
    "baichuan-inc/Baichuan2-7B-Base",
    "baichuan-inc/Baichuan2-7B-Chat",     # â­
    "baichuan-inc/Baichuan2-13B-Base",
    "baichuan-inc/Baichuan2-13B-Chat",    # â­
]
```

### EleutherAIç³»åˆ—

**Decoder-Only**

```python
models = [
    "EleutherAI/pythia-1b",
    "EleutherAI/pythia-3b",
    "EleutherAI/pythia-7b",
    "EleutherAI/pythia-12b",
    "EleutherAI/gpt-neo-2.7B",
    "EleutherAI/gpt-j-6B",
]
```

### BigCodeç³»åˆ—

**Decoder-Only (ä»£ç ä¼˜åŒ–)**

```python
models = [
    "bigcode/starcoder",                  # 15Bä»£ç æ¨¡å‹
    "bigcode/starcoder2",                 # æ–°ç‰ˆæœ¬
]
```

### å…¶ä»–å¼€æºæ¨¡å‹

```python
# MPTç³»åˆ—
models = [
    "mosaicml/mpt-7b",
    "mosaicml/mpt-30b",
]

# Falconç³»åˆ—
models = [
    "tiiuae/falcon-7b",
    "tiiuae/falcon-40b",
]

# Bloomç³»åˆ—
models = [
    "bigscience/bloom-560m",
    "bigscience/bloom-1b7",
    "bigscience/bloom-3b",
    "bigscience/bloom-7b1",
]

# OPTç³»åˆ—
models = [
    "facebook/opt-125m",
    "facebook/opt-350m",
    "facebook/opt-1.3b",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b",
    "facebook/opt-13b",
]
```

## æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ¨ç†é€Ÿåº¦ | ç²¾åº¦ | æ¨èåœºæ™¯ |
|------|-------|--------|------|--------|
| flan-t5-small | 80M | ğŸ”¥ğŸ”¥ğŸ”¥ | â˜…â˜… | å¿«é€ŸåŸå‹ |
| flan-t5-base | 250M | ğŸ”¥ğŸ”¥ | â˜…â˜…â˜… | â­ é€šç”¨æ¨è |
| flan-t5-large | 780M | ğŸ”¥ | â˜…â˜…â˜…â˜… | é«˜ç²¾åº¦éœ€æ±‚ |
| phi-2 | 2.7B | ğŸ”¥ğŸ”¥ğŸ”¥ | â˜…â˜…â˜… | è½»é‡çº§ |
| llama-2-7b | 7B | ğŸ”¥ | â˜…â˜…â˜…â˜… | é€šç”¨ |
| llama-2-13b | 13B | ğŸ”¥ | â˜…â˜…â˜…â˜…â˜… | â­ ç²¾åº¦ä¼˜å…ˆ |
| flan-t5-xl | 3B | ğŸ”¥ | â˜…â˜…â˜…â˜… | ä¸­ç­‰ç²¾åº¦ |
| mistral-7b | 7B | ğŸ”¥ | â˜…â˜…â˜…â˜…â˜… | â­ é€Ÿåº¦vsç²¾åº¦ |

## å‚æ•°æ¨è

### å¿«é€Ÿåº”ç”¨ (å»¶è¿Ÿ< 1ç§’)
```python
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="google/flan-t5-base",
    k=5,
    num_child=2,  # å‡å°‘æ¯”è¾ƒæ¬¡æ•°
)
```

### å¹³è¡¡æ–¹æ¡ˆ (å»¶è¿Ÿ< 5ç§’)
```python
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="google/flan-t5-large",
    k=10,
    num_child=3,  # æ ‡å‡†å€¼
    method="heapsort",
)
```

### é«˜ç²¾åº¦æ–¹æ¡ˆ (å»¶è¿Ÿ< 30ç§’)
```python
from llmrankers.universal_setwise import UniversalSetwiseLlmRankerWithVoting

ranker = UniversalSetwiseLlmRankerWithVoting(
    model_name_or_path="meta-llama/Llama-2-13b-chat-hf",
    k=10,
    num_child=4,  # æ›´å¤šæ¯”è¾ƒ
    num_permutation=3,  # æŠ•ç¥¨å¢å¼º
)
```

## é‡åŒ–é€‰é¡¹

å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
# 8-bité‡åŒ– (éœ€è¦ bitsandbytes)
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="meta-llama/Llama-2-70b-hf",
    device="cuda"
    # æ¨¡å‹ä¼šè‡ªåŠ¨8-bité‡åŒ–
)

# æˆ–ä½¿ç”¨å·²ç»é‡åŒ–çš„æ¨¡å‹
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="TheBloke/Llama-2-7B-GGUF",
)
```

## è®¿é—®é™åˆ¶æ¨¡å‹

æŸäº›æ¨¡å‹ï¼ˆå¦‚LLaMAï¼‰éœ€è¦HuggingFace Hubæˆæƒï¼š

```bash
# 1. åœ¨ https://huggingface.co ç”³è¯·è®¿é—®æƒé™
# 2. è·å–API token
huggingface-cli login

# 3. ä½¿ç”¨æ¨¡å‹
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="meta-llama/Llama-2-7b-chat-hf"
)
```

## æœ¬åœ°æ¨¡å‹

```python
# ä½¿ç”¨æœ¬åœ°ä¿å­˜çš„æ¨¡å‹
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="/path/to/local/model",
)
```

## ç‰ˆæœ¬ç®¡ç†

æŒ‡å®šç‰¹å®šç‰ˆæœ¬ï¼ˆä½¿ç”¨revisionå‚æ•°ï¼‰ï¼š

```python
# ä½¿ç”¨ç‰¹å®šcommit/tag
ranker = UniversalSetwiseLlmRanker(
    model_name_or_path="google/flan-t5-base",
    # revisionå¯ä»¥åœ¨AutoConfigä¸­é…ç½®
)
```

---

**æœ€åæ›´æ–°**: 2024å¹´1æœˆ
**æ”¯æŒæ¨¡å‹æ•°é‡**: 100+
**è‡ªåŠ¨é€‚é…**: æ˜¯ âœ“
